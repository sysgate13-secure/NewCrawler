import requests
import bleach
from bs4 import BeautifulSoup
from datetime import datetime
from sqlalchemy.orm import Session
from app.models import News, Wiki
from app.ai_summarizer import summarize_news, generate_wiki_content
import time
import re

# ë³´ì•ˆ í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤
SECURITY_KEYWORDS = {
    'ì•…ì„±ì½”ë“œ': ['ì•…ì„±', 'ë©€ì›¨ì–´', 'malware', 'ëœì„¬', 'ransomware', 'ë°”ì´ëŸ¬ìŠ¤', 'virus', 'trojan', 'íŠ¸ë¡œì´ëª©ë§ˆ', 'ì›œ', 'worm', 'í”¼ì‹±', 'phishing', 'ìŠ¤í”¼ì–´', 'spear', 'í•´ì»¤', 'hacker', 'í•´í‚¹', 'hacking'],
    'ì·¨ì•½ì ': ['ì·¨ì•½', 'cve', 'ì·¨ì•½ì ', 'ì·¨ì•½ì„±', 'vulnerability', 'ì œë¡œë°ì´', 'zero-day', 'exploit', 'ìµìŠ¤í”Œë¡œì‡', 'ë³´ì•ˆíŒ¨ì¹˜', 'íŒ¨ì¹˜', 'patch', 'ë²„ê·¸', 'bug'],
    'ì•”í˜¸í™”': ['ì•”í˜¸', 'crypto', 'ì•”í˜¸í•™', 'ì•”í˜¸í™”', 'encryption', 'rsa', 'aes', 'sha', 'í‚¤ë…¸ì¶œ', 'í•´ì‹œ', 'hash', 'ì¸ì¦ì„œ', 'certificate'],
    'ë„¤íŠ¸ì›Œí¬': ['ë„¤íŠ¸ì›Œí¬', 'network', 'ë°©í™”ë²½', 'firewall', 'ddos', 'ë””ë„ìŠ¤', 'botnet', 'ë´‡ë„·', 'ìŠ¤ìº”', 'scan', 'í¬íŠ¸', 'port', 'íŒ¨í‚·', 'packet'],
    'ì›¹ë³´ì•ˆ': ['ì›¹', 'web', 'xss', 'sql', 'sqlinjection', 'csrf', 'injection', 'ì¸ì ì…˜', 'íŒŒì¼ì—…ë¡œë“œ', 'ê²½ë¡œíƒìƒ‰', 'ì¿ í‚¤', 'cookie', 'ì„¸ì…˜', 'session'],
    'ì •ë³´ë³´í˜¸': ['ì •ë³´ë³´í˜¸', 'security', 'ë³´ì•ˆ', 'ì •ë³´ìœ ì¶œ', 'ë°ì´í„°ìœ ì¶œ', 'leak', 'breach', 'ë…¸ì¶œ', 'exposure', 'ì¹¨í•´', 'ì¹¨ì…', 'intrusion', 'ë°©ì–´', 'ëŒ€ì‘'],
}

# ê°„ë‹¨ í‚¤ì›Œë“œ ì¶”ì¶œê¸° (ë³´ì•ˆ í‚¤ì›Œë“œë§Œ í•„í„°ë§)
def extract_keywords(text, top_n=5):
    if not text:
        return []
    t = text.lower()
    # ë³´ì•ˆ í‚¤ì›Œë“œ ì¶”ì¶œ
    found_keywords = []
    for category, keywords_list in SECURITY_KEYWORDS.items():
        for kw in keywords_list:
            if kw in t:
                found_keywords.append(kw)
    
    # ì¤‘ë³µ ì œê±° ë° ë¹ˆë„ ê³„ì‚°
    freq = {}
    for kw in found_keywords:
        freq[kw] = freq.get(kw, 0) + 1
    
    # ìƒìœ„ top_n ì„ íƒ
    if freq:
        items = sorted(freq.items(), key=lambda x: x[1], reverse=True)
        return [w for w, _ in items[:top_n]]
    return []


# í…ìŠ¤íŠ¸ ìš”ì•½ í•¨ìˆ˜ (3-5ì¤„ë¡œ ìë™ ìš”ì•½)
def summarize_text(text, target_length=100):
    """
    í…ìŠ¤íŠ¸ë¥¼ 3-5ì¤„ë¡œ ìš”ì•½í•©ë‹ˆë‹¤.
    """
    if not text or len(text) < 50:
        return text
    
    # ë¬¸ì¥ ë¶„ë¦¬
    sentences = re.split(r'(?<=[.!?])\s+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # ë¬¸ì¥ì´ 3-5ê°œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    if 3 <= len(sentences) <= 5:
        return ' '.join(sentences)
    
    # ë¬¸ì¥ì´ ë„ˆë¬´ ë§ìœ¼ë©´ ì²˜ìŒ 4-5ê°œ ë¬¸ì¥ìœ¼ë¡œ ìë¥´ê¸°
    if len(sentences) > 5:
        # ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ê³ ë ¤í•´ì„œ 3-5ê°œ ì„ íƒ
        total_length = 0
        selected = []
        for sent in sentences:
            if total_length + len(sent) > target_length and len(selected) >= 3:
                break
            selected.append(sent)
            total_length += len(sent)
        return ' '.join(selected) if selected else ' '.join(sentences[:5])
    
    # ë¬¸ì¥ì´ 1-2ê°œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    return ' '.join(sentences)


# ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘ (ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ ì²´í¬)
CATEGORY_KEYWORDS = [
    # ì·¨ì•½ì  ê´€ë ¨ í‚¤ì›Œë“œëŠ” ë†’ì€ ìš°ì„ ìˆœìœ„ë¡œ ê²€ì‚¬
    ('vulnerability', ['ì·¨ì•½', 'cve', 'ì·¨ì•½ì ', 'ì·¨ì•½ì„±', 'ì œë¡œë°ì´', 'zero-day', 'exploit', 'ìµìŠ¤í”Œë¡œì‡', 'ë³´ì•ˆíŒ¨ì¹˜', 'íŒ¨ì¹˜']),
    # ëœì„¬/ì•…ì„±ì½”ë“œ/í”¼ì‹± ë“± ìœ„í˜‘(TTP) ê´€ë ¨
    ('malware', ['ì•…ì„±', 'ë©€ì›¨ì–´', 'malware', 'ëœì„¬', 'ransom', 'ransomware', 'ë°”ì´ëŸ¬ìŠ¤', 'trojan', 'ì•…ì„±ì½”ë“œ', 'í”¼ì‹±', 'phishing', 'ìŠ¤í”¼ì–´í”¼ì‹±', 'í•´í‚¹', 'hacking']),
    # ë„¤íŠ¸ì›Œí¬/ì¸í”„ë¼
    ('network', ['ë„¤íŠ¸ì›Œí¬', 'ë°©í™”ë²½', 'ë¼ìš°í„°', 'ìŠ¤ìœ„ì¹˜', 'íŒ¨í‚·', 'tcp', 'udp', 'dDoS', 'ë””ë„ìŠ¤', 'ë””ë„ìŠ¤ê³µê²©', 'ë„¤íŠ¸ì›Œí¬ì¥ì• ']),
    # ì›¹ ê´€ë ¨ ì·¨ì•½ì /ê³µê²©
    ('web', ['ì›¹', 'ì‚¬ì´íŠ¸', 'xss', 'sql', 'csrf', 'injection', 'sql-injection', 'cross-site', 'íŒŒì¼ì—…ë¡œë“œ', 'ê²½ë¡œíƒìƒ‰', 'directory traversal']),
    # ì•”í˜¸/ì•”í˜¸í™” ê´€ë ¨
    ('crypto', ['ì•”í˜¸', 'crypto', 'crypt', 'ì•”í˜¸í•™', 'ì•”í˜¸í™”', 'rsa', 'aes', 'sha', 'í‚¤ë…¸ì¶œ', 'í‚¤ íƒˆì·¨']),
    # ë°ì´í„° ìœ ì¶œ/ì •ë³´ë…¸ì¶œì€ trend/incidentë¡œ ì²˜ë¦¬ë  ìˆ˜ ìˆìŒ â€” ìš°ì„  'trend'ë¡œ ë¶„ë¥˜
    ('trend', ['ìœ ì¶œ', 'ë°ì´í„°ìœ ì¶œ', 'ì •ë³´ìœ ì¶œ', 'leak', 'exposure', 'breach']),
]

# ì¹´í…Œê³ ë¦¬ ë¼ë²¨
CATEGORY_LABELS = {
    'malware': 'ì•…ì„±ì½”ë“œ',
    'vulnerability': 'ì·¨ì•½ì ',
    'network': 'ë„¤íŠ¸ì›Œí¬',
    'web': 'ì›¹ ë³´ì•ˆ',
    'crypto': 'ì•”í˜¸í•™',
    'trend': 'ê¸°íƒ€'
}


def determine_category(text: str) -> str:
    if not text:
        return 'trend'
    t = text.lower()
    for cat, keys in CATEGORY_KEYWORDS:
        for k in keys:
            if k in t:
                return cat
    return 'trend'

def crawl_boannews(db: Session):
    """ë³´ì•ˆë‰´ìŠ¤ í¬ë¡¤ë§"""
    url = "https://www.boannews.com/media/t_list.asp"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'euc-kr'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        count = 0
        news_items = soup.select('div.news_list')
        
        for item in news_items[:15]:
            try:
                # a íƒœê·¸ì—ì„œ ë§í¬ ì¶”ì¶œ
                link_tag = item.select_one('a')
                if not link_tag:
                    continue
                
                link = link_tag.get('href', '')
                if not link.startswith('http'):
                    link = 'https://www.boannews.com' + link
                
                # ì œëª© ì¶”ì¶œ
                news_txt = item.select_one('.news_txt')
                if not news_txt:
                    continue
                
                title = news_txt.get_text(strip=True)
                if not title or len(title) < 5:
                    continue

                # ìƒì„¸ í˜ì´ì§€ì—ì„œ ìš”ì•½(summary) ì¶”ì¶œ ì‹œë„
                summary = ""
                try:
                    article_resp = requests.get(link, headers=headers, timeout=10)
                    article_resp.encoding = 'euc-kr'
                    article_soup = BeautifulSoup(article_resp.text, 'html.parser')

                    # ì‹œë„ ìˆœì„œ: og:description, meta description, ì£¼ìš” ë³¸ë¬¸ì˜ ì²« ë¬¸ë‹¨
                    meta_og = article_soup.find('meta', property='og:description')
                    if meta_og and meta_og.get('content'):
                        summary = meta_og.get('content').strip()
                    else:
                        meta_desc = article_soup.find('meta', attrs={'name': 'description'})
                        if meta_desc and meta_desc.get('content'):
                            summary = meta_desc.get('content').strip()
                        else:
                            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ë³¸ë¬¸ ì„ íƒì ì‹œë„
                            possible_selectors = [
                                'div.view_txt', 'div#view_txt', 'div.article', 'div#article',
                                'div.article_view', 'div.news_view', 'div.content', 'article',
                                '.article_con', '.view_content', '.article-body'
                            ]
                            for sel in possible_selectors:
                                node = article_soup.select_one(sel)
                                if node:
                                    # ë¬¸ë‹¨ë“¤ì„ í•©ì³ì„œ ìš”ì•½ ìƒì„±
                                    p = node.find('p')
                                    if p and p.get_text(strip=True):
                                        summary = p.get_text(strip=True)
                                        break
                            # ë§ˆì§€ë§‰ ëŒ€ì•ˆ: ì²« ë²ˆì§¸ <p> íƒœê·¸
                            if not summary:
                                p_first = article_soup.find('p')
                                if p_first and p_first.get_text(strip=True):
                                    summary = p_first.get_text(strip=True)
                except Exception as e:
                    print(f"ìš”ì•½ ì¶”ì¶œ ì˜¤ë¥˜({title[:30]}...): {e}")
                    summary = ""

                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜: ì¤‘ì•™ ì •ì˜ëœ ê·œì¹™ ì‚¬ìš©
                category = determine_category(title + ' ' + (summary or ''))

                # ì¤‘ë³µ ì²´í¬
                existing = db.query(News).filter(News.url == link).first()
                if existing:
                    continue

                # AI ìš”ì•½ ì‹œë„
                ai_summary = summarize_news(title, summary)
                if ai_summary:
                    processed_summary = ai_summary
                elif summary and len(summary) > 50:
                    # AI ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ìš”ì•½
                    processed_summary = summarize_text(summary)
                else:
                    processed_summary = summary or ""

                news = News(
                    title=bleach.clean(title),
                    source="ë³´ì•ˆë‰´ìŠ¤",
                    date=datetime.now().strftime("%Y-%m-%d"),
                    summary=bleach.clean(processed_summary),
                    category=category,
                    url=link
                )
                db.add(news)
                db.commit()
                
                # Wiki í…Œì´ë¸”ì— ìë™ ì¶”ê°€ (ì œëª© ê¸°ì¤€ ì¤‘ë³µ ë°©ì§€)
                # ìœ„í‚¤ëŠ” ë‰´ìŠ¤ì™€ ë™ì¼í•œ ì½˜í…ì¸ ê°€ ë˜ì§€ ì•Šë„ë¡ í…œí”Œë¦¿í™”í•˜ì—¬ ìƒì„±
                # ì¹´í…Œê³ ë¦¬ ë¼ë²¨ì€ ì „ì—­ ë§¤í•‘ ì‚¬ìš©

                wiki_existing = db.query(Wiki).filter(Wiki.title == title).first()
                if not wiki_existing:
                    # AI ìœ„í‚¤ ì½˜í…ì¸  ìƒì„±
                    wiki_cat = CATEGORY_LABELS.get(category, category or 'ê¸°íƒ€')
                    wiki_content = generate_wiki_content(title, wiki_cat)
                    
                    if not wiki_content:
                        # AI ì‹¤íŒ¨ ì‹œ í´ë°±
                        wiki_content = f"ì¶œì²˜: ë³´ì•ˆë‰´ìŠ¤\nì›ë¬¸: {link}\n\nìš”ì•½:\n{(processed_summary or 'ìš”ì•½ ì—†ìŒ')}"
                    
                    wiki = Wiki(
                        title=bleach.clean(title),
                        category=wiki_cat,
                        preview=(bleach.clean(processed_summary[:200]) + '...') if processed_summary and len(processed_summary) > 200 else (bleach.clean(processed_summary) or ''),
                        content=bleach.clean(wiki_content, tags=['p', 'a', 'strong', 'em', 'ul', 'li', 'h1', 'h2', 'h3']),
                        type="auto"
                    )
                    db.add(wiki)
                    db.commit()
                
                count += 1
                print(f"ì¶”ê°€: {title[:50]}...")
                
            except Exception as e:
                print(f"í•­ëª© íŒŒì‹± ì˜¤ë¥˜: {e}")
                continue
        
        db.commit()
        return count
        
    except Exception as e:
        print(f"ë³´ì•ˆë‰´ìŠ¤ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return 0

def crawl_krcert(db: Session):
    """KrCERT ë³´ì•ˆê³µì§€ í¬ë¡¤ë§"""
    url = "https://www.krcert.or.kr/data/secNoticeList.html"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # KrCERT í…Œì´ë¸”ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ
        table = soup.find('table', class_='artclTable')
        if table:
            rows = table.find_all('tr')[1:11]  # ìƒìœ„ 10ê°œ
            count = 0
            
            for row in rows:
                try:
                    cols = row.find_all('td')
                    if len(cols) < 2:
                        continue
                    
                    title_elem = cols[0].find('a')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get('href', '')
                    
                    if not link.startswith('http'):
                        link = 'https://www.krcert.or.kr' + link
                    
                    if not title or len(title) < 5:
                        continue
                    
                    # ì¤‘ë³µ ì²´í¬
                    existing = db.query(News).filter(News.url == link).first()
                    if existing:
                        continue
                    
                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ìš”ì•½ ì¶”ì¶œ
                    summary = ""
                    try:
                        article_resp = requests.get(link, headers=headers, timeout=10)
                        article_resp.encoding = 'utf-8'
                        article_soup = BeautifulSoup(article_resp.text, 'html.parser')
                        
                        content_div = article_soup.find('div', class_='cont')
                        if content_div:
                            p = content_div.find('p')
                            if p:
                                summary = p.get_text(strip=True)[:300]
                    except:
                        pass
                    
                    category = determine_category(title + ' ' + (summary or ''))
                    
                    news = News(
                        title=title,
                        url=link,
                        source="KrCERT",
                        summary=summary or "",
                        category=category
                    )
                    db.add(news)
                    db.commit()
                    
                    count += 1
                    print(f"KrCERT ì¶”ê°€: {title[:50]}...")
                    time.sleep(0.5)
                    
                except Exception as e:
                    print(f"KrCERT í•­ëª© ì˜¤ë¥˜: {e}")
                    continue
            
            return count
        
        return 0
        
    except Exception as e:
        print(f"KrCERT í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return 0

def crawl_zdnet(db: Session):
    """ZDNet ë³´ì•ˆ ë‰´ìŠ¤ í¬ë¡¤ë§"""
    url = "https://www.zdnet.co.kr/news/security/"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ZDNet ê¸°ì‚¬ ì°¾ê¸°
        articles = soup.find_all('article', class_='card-item')[:10]
        count = 0
        
        for article in articles:
            try:
                title_elem = article.find('h2')
                link_elem = article.find('a')
                
                if not title_elem or not link_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = link_elem.get('href', '')
                
                if not link.startswith('http'):
                    link = 'https://www.zdnet.co.kr' + link
                
                if not title or len(title) < 5:
                    continue
                
                # ì¤‘ë³µ ì²´í¬
                existing = db.query(News).filter(News.url == link).first()
                if existing:
                    continue
                
                # ìš”ì•½ ì¶”ì¶œ
                summary_elem = article.find('p', class_='desc')
                summary = summary_elem.get_text(strip=True) if summary_elem else ""
                
                category = determine_category(title + ' ' + (summary or ''))
                
                news = News(
                    title=title,
                    url=link,
                    source="ZDNet",
                    summary=summary[:300] if summary else "",
                    category=category
                )
                db.add(news)
                db.commit()
                
                count += 1
                print(f"ZDNet ì¶”ê°€: {title[:50]}...")
                time.sleep(0.5)
                
            except Exception as e:
                print(f"ZDNet í•­ëª© ì˜¤ë¥˜: {e}")
                continue
        
        return count
        
    except Exception as e:
        print(f"ZDNet í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return 0

def crawl_cisa(db: Session):
    """CISA (ë¯¸êµ­ ì‚¬ì´ë²„ë³´ì•ˆì²­) ê³µì§€ í¬ë¡¤ë§"""
    url = "https://www.cisa.gov/news-events/alerts"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # CISA ì•Œë¦¼ ì°¾ê¸°
        alerts = soup.find_all('div', class_='alert-item')[:10]
        count = 0
        
        for alert in alerts:
            try:
                title_elem = alert.find('h3')
                link_elem = alert.find('a')
                
                if not title_elem or not link_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = link_elem.get('href', '')
                
                if not link.startswith('http'):
                    link = 'https://www.cisa.gov' + link
                
                if not title or len(title) < 5:
                    continue
                
                # ì¤‘ë³µ ì²´í¬
                existing = db.query(News).filter(News.url == link).first()
                if existing:
                    continue
                
                summary_elem = alert.find('p')
                summary = summary_elem.get_text(strip=True) if summary_elem else ""
                
                category = determine_category(title + ' ' + (summary or ''))
                
                news = News(
                    title=title,
                    url=link,
                    source="CISA",
                    summary=summary[:300] if summary else "",
                    category=category
                )
                db.add(news)
                db.commit()
                
                count += 1
                print(f"CISA ì¶”ê°€: {title[:50]}...")
                time.sleep(0.5)
                
            except Exception as e:
                print(f"CISA í•­ëª© ì˜¤ë¥˜: {e}")
                continue
        
        return count
        
    except Exception as e:
        print(f"CISA í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return 0


# ë²”ìš© í¬ë¡¤ëŸ¬ í—¬í¼ ë° í•´ì™¸ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ë“¤
def _generic_crawl(db: Session, list_url: str, domain: str, source_label: str, 
                   title_selector: str = None, summary_selector: str = None, max_items: int = 8):
    """ë²”ìš© í¬ë¡¤ëŸ¬: ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ í›„ ì œëª©/ìš”ì•½ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    try:
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        resp = requests.get(list_url, headers=headers, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, 'html.parser')

        links = []
        # ì…€ë ‰í„°ê°€ ì œê³µëœ ê²½ìš° í•´ë‹¹ ìš”ì†Œë“¤ì—ì„œ ë§í¬ ì¶”ì¶œ
        if title_selector:
            for item in soup.select(title_selector):
                a = item if item.name == 'a' else item.find('a')
                if a and a.get('href'):
                    href = a['href']
                    if href.startswith('/'): href = requests.compat.urljoin(list_url, href)
                    if domain in href and href not in links: links.append(href)
        else:
            # ê¸°ë³¸ ë§í¬ ì¶”ì¶œ ë¡œì§
            for a in soup.find_all('a', href=True):
                href = a['href']
                if href.startswith('/'): href = requests.compat.urljoin(list_url, href)
                if domain in href and href not in links: links.append(href)

        selected = []
        for l in links:
            if any(x in l for x in ['#', '/tag/', '/category/', '/comments', '/page/', '?']): continue
            if l not in selected: selected.append(l)
            if len(selected) >= max_items: break

        added = 0
        for link in selected:
            try:
                aresp = requests.get(link, headers=headers, timeout=10)
                aresp.raise_for_status()
                asoup = BeautifulSoup(aresp.text, 'html.parser')

                # ì œëª© ì¶”ì¶œ
                title = None
                meta_og = asoup.find('meta', property='og:title')
                if meta_og: title = meta_og.get('content', '').strip()
                if not title:
                    h1 = asoup.find('h1')
                    if h1: title = h1.get_text(strip=True)
                if not title: continue

                # ìš”ì•½ ì¶”ì¶œ
                summary = ''
                # 1. ì¸ìë¡œ ë°›ì€ ìš”ì•½ ì…€ë ‰í„° ì‹œë„ (ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ë” ë‚˜ì„ ìˆ˜ë„ ìˆìœ¼ë‚˜ ì—¬ê¸°ì„  ìƒì„¸ í˜ì´ì§€ ê¸°ì¤€)
                if summary_selector:
                    s_node = asoup.select_one(summary_selector)
                    if s_node: summary = s_node.get_text(strip=True)
                
                if not summary:
                    meta_desc = asoup.find('meta', attrs={'name': 'description'}) or asoup.find('meta', property='og:description')
                    if meta_desc: summary = meta_desc.get('content', '').strip()
                
                if not summary:
                    p = asoup.find('p')
                    if p: summary = p.get_text(strip=True)

                category = determine_category(title + ' ' + (summary or ''))
                existing = db.query(News).filter(News.url == link).first()
                if existing: continue

                # AI ìš”ì•½ ì‹œë„
                ai_summary = summarize_news(title, summary)
                if ai_summary:
                    processed_summary = ai_summary
                else:
                    processed_summary = summarize_text(summary) if summary else ""

                news = News(
                    title=bleach.clean(title), source=source_label, date=datetime.now().strftime("%Y-%m-%d"),
                    summary=bleach.clean(processed_summary),
                    category=category, url=link
                )
                db.add(news)
                db.commit()

                # ìœ„í‚¤ ìë™ ìƒì„±
                wiki_existing = db.query(Wiki).filter(Wiki.title == title).first()
                if not wiki_existing:
                    wiki_cat = CATEGORY_LABELS.get(category, 'ê¸°íƒ€')
                    wiki_content = generate_wiki_content(title, wiki_cat)
                    
                    if not wiki_content:
                        wiki_content = f"ì¶œì²˜: {source_label}\nì›ë¬¸: {link}\n\nìš”ì•½:\n{processed_summary or 'ìš”ì•½ ì—†ìŒ'}"
                        
                    wiki = Wiki(
                        title=bleach.clean(title), category=wiki_cat,
                        preview=(bleach.clean(processed_summary[:200]) + '...') if processed_summary and len(processed_summary) > 200 else (bleach.clean(processed_summary) or ''),
                        content=bleach.clean(wiki_content, tags=['p', 'a', 'strong', 'em', 'ul', 'li', 'h1', 'h2', 'h3']),
                        type="auto"
                    )
                    db.add(wiki)
                    db.commit()

                added += 1
                print(f"{source_label} ì¶”ê°€: {title[:50]}...")
                time.sleep(0.5)
            except Exception as e:
                print(f"{source_label} í•­ëª© ì˜¤ë¥˜: {e}")
                continue
        return added
    except Exception as e:
        print(f"{source_label} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return -1

def crawl_cyberscoop(db: Session):
    return _generic_crawl(db, 'https://cyberscoop.com/news/', 'cyberscoop.com', 'CyberScoop', 
                        title_selector='.post-item__title-link', summary_selector='.post-item__excerpt')

def crawl_helpnetsecurity(db: Session):
    return _generic_crawl(db, 'https://www.helpnetsecurity.com/view/news/', 'helpnetsecurity.com', 'HelpNetSecurity', 
                        title_selector='.card-title a')

def crawl_hackread(db: Session):
    return _generic_crawl(db, 'https://hackread.com/', 'hackread.com', 'HackRead', 
                        title_selector='.cs-entry__title a', summary_selector='.cs-entry__excerpt')

def crawl_infosecurity(db: Session):
    return _generic_crawl(db, 'https://www.infosecurity-magazine.com/news/', 'infosecurity-magazine.com', 'InfoSecurity', 
                        title_selector='.webpage-title a', summary_selector='.webpage-summary')

def crawl_all(db: Session):
    """ëª¨ë“  ì†ŒìŠ¤ í¬ë¡¤ë§"""
    start_time = datetime.now()
    print(f"\n[ğŸš€] {start_time.strftime('%Y-%m-%d %H:%M:%S')} - í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("==================================================")
    
    total = 0

    print("\n[1/2] êµ­ë‚´ ë³´ì•ˆë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
    res = crawl_boannews(db)
    if res != -1: 
        total += res
        print(f"   âœ… {res}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        print("   âŒ ìˆ˜ì§‘ ì‹¤íŒ¨")
    time.sleep(1)

    print("\n[2/2] HackRead ìˆ˜ì§‘ ì¤‘...")
    try:
        r = crawl_hackread(db)
        if r != -1: 
            total += r
            print(f"   âœ… {r}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        else:
            print("   âŒ ìˆ˜ì§‘ ì‹¤íŒ¨")
        time.sleep(1)
    except Exception as e:
        print(f"   âŒ HackRead ì˜¤ë¥˜: {e}")

    end_time = datetime.now()
    duration = end_time - start_time
    minutes, seconds = divmod(duration.seconds, 60)
    
    print("\n==================================================")
    print(f"[âœ…] {end_time.strftime('%Y-%m-%d %H:%M:%S')} - í¬ë¡¤ë§ ì™„ë£Œ")
    print(f"[â±ï¸] ì´ ì†Œìš” ì‹œê°„: {minutes}ë¶„ {seconds}ì´ˆ")
    print(f"[ğŸ“Š] ìƒˆë¡œ ì¶”ê°€ëœ ë‰´ìŠ¤: ì´ {total}ê°œ")
    print("==================================================\n")
    return total

if __name__ == "__main__":
    from app.database import SessionLocal
    db = SessionLocal()
    crawl_all(db)
    db.close()
